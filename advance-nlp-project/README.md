In this project, we attempt to test and compare the strengths of various natural language processing and machine learning models in generating a summarization of a given text document. We explore prediction models from data-based analysis using Zipfâ€™s Law and SVM, leading to feature-based analysis using NLP techniques like Lemmatization, TF-IDF and Word2Vec and finally conclude by training an LSTM. This deduces the challenges with each model and justifies the authenticity and
flexibility of the final approach. We use a dataset of 2.5 Gigabytes of various articles labeled with some small summarization. Our baseline models were trained on very small toy data-sets since they were slower as compared to the other models we explored with the entire dataset.
