{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = 'trn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.9'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN0 = 'vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN1 = 'trn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlend=25 # 0 - if we dont want to use description at all\n",
    "maxlenh=25\n",
    "maxlen = maxlend + maxlenh\n",
    "rnn_size = 512 # must be same as 160330-word-gen\n",
    "rnn_layers = 3  # match FN1\n",
    "batch_norm=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_rrn_size = 40 if maxlend else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "seed=42\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "optimizer = 'adam'\n",
    "LR = 1e-4\n",
    "# batch_size=64\n",
    "batch_size=16\n",
    "nflips=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_train_samples = 30000\n",
    "nb_train_samples = 10000\n",
    "# nb_val_samples = 3000\n",
    "nb_val_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/%s.pkl'%FN0, 'rb') as fp:\n",
    "    embedding, idx2word, word2idx, glove_idx2idx = pickle.load(fp, encoding='latin1')\n",
    "vocab_size, embedding_size = embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/%s.data.pkl'%FN0, 'rb') as fp:\n",
    "    X, Y = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_unknown_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:len(Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples 10000 10000\n",
      "dimension of embedding space for words 100\n",
      "vocabulary size 40000 the last 10 words can be used as place holders for unknown/oov words\n",
      "total number of different words 296866 296866\n",
      "number of words outside vocabulary which we can substitue using glove similarity 47685\n",
      "number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov) 209181\n"
     ]
    }
   ],
   "source": [
    "print('number of examples',len(X),len(Y))\n",
    "print('dimension of embedding space for words',embedding_size)\n",
    "print('vocabulary size', vocab_size, 'the last %d words can be used as place holders for unknown/oov words'%nb_unknown_words)\n",
    "print('total number of different words',len(idx2word), len(word2idx))\n",
    "print('number of words outside vocabulary which we can substitue using glove similarity', len(glove_idx2idx))\n",
    "print('number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov)',len(idx2word)-vocab_size-len(glove_idx2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nb_unknown_words):\n",
    "    idx2word[vocab_size - 1 - i] = '<%d>'%i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov0 = vocab_size - nb_unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(oov0, len(idx2word)):\n",
    "    idx2word[i] = idx2word[i]+'^'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9000, 9000, 1000, 1000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=nb_val_samples, random_state=seed)\n",
    "len(X_train), len(Y_train), len(X_test), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = 0\n",
    "eos = 1\n",
    "idx2word[empty] = '_'\n",
    "idx2word[eos] = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import random, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbCallback = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "saveModelCallback = keras.callbacks.ModelCheckpoint('./checkpoints/Iweights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prt(label, x):\n",
    "    print(label+':', end='')\n",
    "    for w in x:\n",
    "        print(idx2word[w], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:Join @AlertLogic^ at @DevOpsSummit Silicon Valley | #DevOps^ #BigData^ #API^ #Containers^ \n",
      "D:SYS-CON Events announced today that Alert Logic,^ the leading provider of Security-as-a-Service^ solutions for the cloud, has been named \"Bronze^ Sponsor\"^ of SYS-CON's 17th International Cloud Expo ® and DevOps Summit 2015 Silicon Valley, which will take place November 3-5, 2015, at the Santa Clara Convention Center in Santa Clara, CA. Alert Logic provides Security-as-a-Service^ for on-premises,^ cloud, and hybrid IT infrastructures,^ delivering deep security insight and continuous protection for customers at a lower cost than traditional security solutions. Fully managed by a team of security and compliance experts, the Alert Logic Security-as-a-Service^ solution provides network, system and web application protection immediately, wherever an organization's IT infrastructure resides.^ Alert Logic partners with all major cloud platforms and hosting providers to protect over 3,000 organizations worldwide. Built for cloud scale, Alert Logic's^ patented platform stores petabytes^ of data, analyzes over 450 million events and identifies over 60,000 security incidents each month, which are managed by a 24×7^ Security Operations Center. Alert Logic,^ founded in 2002, is headquartered in Houston, Texas, with offices in Seattle, Dallas, Cardiff, Belfast and London. For more information, please visit www.alertlogic.com^ . Proactive^ vs. Reactive - A Better Approach to Cloud Security — November 3, 11:00 - 11:35^ am By Stephen Coty^ Organizations from small to large are increasingly adopting cloud solutions to deliver essential business services at a much lower cost. According to cyber security experts, the frequency and severity of cyber-attacks^ are on the rise, causing alarm to businesses and customers across a variety of industries. To defend against exploits like these, a company must adopt a comprehensive security defense strategy that is designed for their business. In 2015, organizations such as United Airlines, Sony Corp, Anthem and the Office of Personnel Management experienced data breaches through malware and other targeted attacks. How will your business adapt to this shift in technology while keeping its security posture?^ In his session at 17th Cloud Expo , Stephen Coty,^ Chief Security Evangelist at Alert Logic,^ will take you into the evolving world of cybercrime^ and give you an insider's^ look into the tactics utilized by fast, well-funded^ and organized attackers.^ He will review real examples and case studies of recent breaches,^ and you will learn expert strategies for protecting your information in the cloud. Discussion includes: An expert profile of today's evolved hackers and threat landscape. What attackers are really after, why, and how they're getting what they want. Strategies you can implement in the cloud to protect against attacks. Speaker Bio Stephen Coty^ is the Chief Security Evangelist at Alert Logic in Houston TX. He formerly ran the Threat Research team, building threat content and delivering threat intelligence to our partners and customers. 'Security^ Built for the Cloud'^ Demo — November 4, 3:40 - 4:15 pm By Steve Coty^ and Paul Fletcher In their Live Hack\"^ presentation at 17th Cloud Expo , Stephen Coty^ and Paul Fletcher, Chief Security Evangelists^ at Alert Logic,^ will provide the audience with a chance to see a live demonstration of the common tools cyber attackers use to attack cloud and traditional IT systems. This \"Live^ Hack\"^ uses open source attack tools that are free and available for download by anybody. Attendees will learn where to find and how to operate these tools for the purpose of testing their own IT infrastructure. They will also witness a cyber-attack^ from both sides - attacker and defender.^ Get an inside view how indicators of compromise are researched to develop security content to be deployed for detection based on these attacks. Speaker Bios^ Stephen Coty^ is the Chief Security Evangelist at Alert Logic in Houston TX. He formally ran the Threat Research team building threat content and delivering threat intelligence to our partners and customers. Paul Fletcher, Chief Security Evangelist at Alert Logic,^ has over 20 years of experience in information technology and security. Register FREE Before Friday!^ ▸ Here Your registration includes: ▸ DevOps sessions ▸ Containers sessions ▸ Microservices sessions The @DevOpsSummit at Cloud Expo - to be held November 3-5, 2015, at the Santa Clara Convention Center in Santa Clara, California - will expand the DevOps community, enable a wide sharing of knowledge, and educate delegates and technology providers alike. Recent research has shown that DevOps dramatically reduces development time, the amount of enterprise IT professionals put out fires, and support time generally.^ Time spent on infrastructure development is significantly increased, and DevOps practitioners report more software releases and higher quality. DevOps Summit 2015 Silicon Valley (November 3-5, 2015, Santa Clara Convention Center, CA)^ DevOps Summit 2016 New York (June 7-9,^ 2016, Javits Center, Manhattan)^ Photo: @DevOpsSummit Silicon Valley, November 2014 Speaking Proposals^ Open The 5th International @DevOpsSummit , co-located with 17th International Cloud Expo - being held November 3-5, 2015, at the Santa Clara Convention Center in Santa Clara, CA - announces that its Call for Papers is open. Born out of proven success in agile development, cloud computing, and process automation, DevOps is a macro trend you cannot afford to miss. Submit your speaking proposal today! ▸ Here Sponsorship Opportunities Open @DevOpsSummit , taking place Nov 3-5, 2015, at the Santa Clara Convention Center in Santa Clara, CA, is co-located with 17th Cloud Expo and will feature technical sessions from a rock star conference faculty and the leading industry players in the world. Sponsor @DevOpsSummit ▸ Here Download Show Prospectus^ ▸ Here DevOps Summit Power Panel | Balancing the Three Pillars^ of DevOps [embedded content] In this @DevOpsSummit Power Panel - moderated by Andi^ Mann (founder^ of @Sageable^ ) - Shannon Williams, co-founder of Rancher^ Labs ; Haseeb^ Budhani,^ co-founder and CEO of Soha^ ; and Dalibor^ Siroky,^ Director and co-founder at Plutora^ ; discussed how to balance these three pillars of DevOps,^ where to focus attention (and resources),^ where organizations might slip up with the wrong focus, how to manage change and risk in all three areas, what is possible and what is not, where to start, and especially how new structures, processes, and technologies can help drive a new DevOps culture. Microservices & IoT Power Panel at @DevOpsSummit Buzzword^ Alert: Microservices and IoT at a DevOps conference?^ What could possibly go wrong? [embedded content] In this Power Panel at @DevOpsSummit,^ moderated by Jason Bloomberg,^ president of Intellyx^ , panelists Roberto Medrano,^ Executive Vice President at Akana ; Lori MacVittie,^ Evangelist for F5^ Networks ; and Troy Topnik,^ ActiveState^ 's Technical Product Manager;^ and Otis Gospodnetić,^ founder of Sematext^ ; peeled^ away the buzz and discuss the important architectural principles behind implementing IoT solutions for the enterprise. As remote IoT devices and sensors become increasingly intelligent,^ they become part of our distributed cloud environment, and we must architect and code accordingly. At the very least, you'll have no problem filling in your buzzword^ bingo^ cards. SYS-CON.tv^ Interviews By Conference Chair Roger Strukhoff^ Otis Gospodnetic^ of Sematext^ ▸ Video Dalibor^ Siroky^ of Plutora^ ▸ Video Charles Kendrick of Isomorphic^ Software ▸ Video Jeremy Steinert^ of WSM^ International ▸ Video Containers & Microservices Expo To Be Colocated^ with @DevOpsSummit Silicon Valley, November 3-5, 2015 at the Santa Clara Convention Center, CA SYS-CON Events announced on June 9, 2015 at the Javits Center that the 2nd \"Containers^ & Microservices Conference\"^ will take place November 3-5, 2015, at the Santa Clara Convention Center, Santa Clara, CA, and the \"Third^ Containers & Microservices Conference\"^ will take place June 7-9,^ 2016, at Javits Center in New York City. Containers and microservices have become topics of intense interest throughout the cloud developer and enterprise IT communities. Microservices focuses on the business and technology of the software architecture design pattern, in which complex applications are composed of small, independent processes communicating with each other using language-agnostic^ APIs.^ Containers are not being considered for the first time by the cloud community, but a current era of re-consideration^ has pushed them to the top of the cloud agenda. Rather than just stuff an OS into a container,^ for example, developers and deployers^ should consider a spectrum of microservices and what they can do. New York and Silicon Valley Sponsors and Exhibitors^ During our last New York and Silicon Valley events, over 12,000 (audited)^ delegates registered and participated in the world's largest DevOps , Containers , and Microservices show, colocated^ with Cloud Expo . Our conference delegates met with over 150 of the world's leading technology pioneers that were among the sponsors and exhibitors,^ including: SYS-CON Media CEO Carmen Gonzalez (@GonzalezCarmen)^ at the Javits Center, Manhattan the day before @DevOpsSummit New York 2015 opens Acision,^ Actifio,^ ActiveState,^ AgilePoint,^ AIC^ , Akana,^ AlertLogic,^ Ambernet,^ Amplidata,^ Apacer^ Memory America Inc., Appcore,^ AppDynamics,^ AppZero,^ Aria^ Systems, Arista^ Networks, Automic,^ Avere^ Systems, Axis Communications, B2CLOUD,^ Basic6,^ Bestwebdesignagencies.com,^ Bitium,^ Blue Box , BMC,^ BroadSoft,^ Brother , Bsquare,^ BUMI,^ CA, Inc., Calm.io,^ CenturyLink,^ Ciqada,^ CiRBA,^ Cisco, Cloudant,^ an IBM Company, Cloudian,^ CoalFire,^ CodeFutures,^ COLUMN^ Technologies, CommVault,^ connect2.me,^ Connected Data, CrashPlan/Code42,^ Creative Business Solutions , Cynny^ Italia^ S.r.l,^ Dasher,^ dcVAST,^ DEAC,^ Dell,^ DevOps.com,^ Distrix^ , DragonGlass,^ Dyn,^ Edgecast^ , ElasticBox,^ Emcien,^ Endstream^ Communications/Open^ Data Centers, EnterpriseDB,^ e-SignLive,^ by Silanis,^ Esri,^ Evident.io,^ FierceDevOps,^ FireHost,^ Genband,^ Gigamon,^ GoodData,^ Gridstore,^ Harbinger^ Group , IAPP,^ IBM, IDenticard^ Access Control, Imperva,^ @DevOpsSummit Demo Theater (June 9-11, 2015, Javits Center)^ on the Expo Floor attracts more delegates than the entire conference of other events IndependenceIT,^ Infor,^ InMage,^ Innodisk,^ Intelligent Systems, Isomorhpic^ , ITinvolve,^ iwNetworks,^ Ixia,^ iXsystems^ , Jelastic,^ Kintone,^ KOTRA^ , Liaison,^ Litmus^ Automation,^ MangoApps,^ Matrix.org,^ MediaTek^ Labs, MetraTech^ (now part of Ericsson),^ Microsoft, Navisite,^ Net Access , Nimble^ Storage,^ NuoDB,^ Inc., Objectivity,^ OMG,^ Open Data Centers, OpenCrowd,^ Optimal^ Design, Oracle,^ OutSystems,^ Parasoft,^ Peak10,^ Peer 1 Hosting,^ PluralSight,^ Plutora,^ ProfitBricks,^ PubNub,^ Quality Technology Services , Quantum,^ Qubell,^ RackWare^ , Rancher^ Labs, Red Hat,^ r-evolutionapp^ , RingStor,^ Robomq.io,^ SafeLogic,^ SAP,^ ScaleMP,^ Seagate,^ Secure Infrastructure & Services, Sematext^ , SendGrid^ , Serena Software, Sherweb,^ SimpleECM,^ Site 24x7,^ Smartvue^ Corporation, SOASTA,^ SoftLayer,^ an IBM Company, SoftwareAG,^ Soha,^ Solgenia,^ SPAN^ Systems, Spirent,^ StackIQ,^ Stateless^ Networks, Storpool,^ Stratogent,^ Stratoscale,^ Supermicro,^ SUSE,^ Tau Institute, Telecity,^ Telehouse,^ Telestax,^ The New York Times , The Vision Times, TierPoint,^ TMCnet,^ Transparent Cloud Computing Consortium,^ Tufin,^ Ulunsoft,^ Utimaco,^ VASCO^ Data Security, Veeam,^ Verizon Enterprise Solutions, Vicom^ Computer Services, VictorOps,^ Virtustream,^ VITRIA^ Technology, Vormetric,^ WHOA.com,^ Will Jaya,^ Windstream,^ WSM^ - Website Movers International, Zentera^ Systems, Zerto.^ @DevOpsSummit New York (June 9-11, 2015) and Silicon Valley (November 3-5, 2015) \"Bronze^ Sponsor\"^ AlertLogic^ Booth at the Javits Center About SYS-CON Media & Events SYS-CON Media ( www.sys-con.com^ ) has since 1994 been connecting technology companies and customers through a comprehensive content stream - featuring over forty focused subject areas, from Cloud Computing to Web Security - interwoven^ with market-leading full-scale conferences produced by SYS-CON Events.^ The company's internationally recognized brands include among others Cloud Expo ® ( @CloudExpo^ ), Big Data Expo ® ( @BigDataExpo^ ), DevOps Summit ( @DevOpsSummit ), @ThingsExpo ® ( @ThingsExpo ), Containers Expo ( @ContainersExpo^ ) and Microservices Expo ( @MicroservicesE^ ). Cloud Expo ® , Big Data Expo ® and @ThingsExpo ® are registered trademarks of Cloud Expo, Inc., a SYS-CON Events company. @DevOpsSummit New York (June 9-11, 2015) and Silicon Valley (November 3-5, 2015) \"Bronze^ Sponsor\"^ Actifio^ Booth at the Javits Center \n"
     ]
    }
   ],
   "source": [
    "i = 334\n",
    "prt('H',Y_train[i])\n",
    "prt('D',X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:Sheriff:^ Texas man killed by deputies appeared to hold knife \n",
      "D:DALLAS^ — A second video that captured Texas deputies fatally shooting a man whose hands were raised appears to show that he was holding a knife, a sheriff said Wednesday, declining to release the video because the investigation is still going on. Bexar County Sheriff Susan Pamerleau said at a news conference that the video has been forwarded to the Texas Department of Public Safety’s^ crime lab to see if the footage can be blown up and slowed down to establish the sequence of Friday’s events. Although it’s unclear from the video what 41-year-old Gilbert Flores may have been holding while facing deputies with his hands up outside of a home near San Antonio, investigators believe it was a knife, she said. “There’s no doubt that what was shown in that video is of grave concern to all of us, but we also want to get this right,”^ Pamerleau said of the investigation, which also involves the FBI.^ She declined to say whether investigators recovered a knife from the scene after the shooting. A separate video taken by a motorist and released publicly shows Flores raise his arms in apparent surrender and stand motionless^ just before the two deputies opened fire, killing him. A utility pole obscured one of his arms in that video, but Pamerleau said the second video, which was taken from a different angle, showed that both of Flores’^ arms were raised when he was shot. “We’re not drawing any conclusions at this point,”^ she said. “That would be inappropriate to do so.”^ The deputies,^ Greg Vasquez^ and Robert Sanchez, have been placed on administrative leave. San Antonio attorney Thomas J. Henry, who is representing the Flores family, previously told The Associated Press that the initial video appears to show that deadly force was unnecessary. “From a lay perspective, seeing the video, it does appear the immediate danger is gone because he had both hands in the air,”^ Henry said. “Now there are other videos and other pieces of evidence that we want to gather.”^ He said the family is considering filing a lawsuit to compel the authorities to turn over more evidence. Juan Contreras,^ president of the Deputy Sheriff’s Association of Bexar County, which is the deputies’^ union, said there are many aspects of the case that haven’t been disclosed by the authorities. “I believe that when the investigation is complete, the members that I represent will be cleared of any wrongdoing,”^ he said in a statement, later adding,^ “I have total confidence in the process and patiently await the outcome of the investigation.”^ Jon Shane,^ a former New Jersey police captain and an associate professor at John Jay College of Criminal Justice in New York City, cautioned that videos of deadly police encounters “almost^ never tell the complete story.”^ He explained that it’s often not immediately apparent what a suspect may have said to officers, whether the suspect may have had more than one weapon or whether the suspect may have been near someone who could have been taken hostage.^ “People seem to have this unwavering sentiment that the video tells the whole story,”^ Shane said. “The reality is that a lot of the time a video raises even more questions.”^ Vasquez^ and Sanchez encountered Flores while responding to a domestic disturbance call. Authorities said they found a woman with a cut and an infant who may have been injured at the home, but they didn’t disclose further details, including the identities of the two or any explanation of how they were hurt. Henry confirmed Wednesday that it was Flores’^ wife and child, but he declined to say whether Flores harmed either of them, only adding that any injuries didn’t result in hospitalization.^ He said any complaints that led to the arrival of deputies “should not have precipitated^ him being shot in the yard.”^ Henry declined to say whether Flores was armed with a knife. “Whether^ he was armed or not armed,^ the issue is going to be whether that scenario was escalating or de-escalating,”^ Henry said. Flores had multiple convictions and had spent 10 years of his life incarcerated,^ according to the San Antonio Express-News.^ Authorities have said deputies used nonlethal^ force in an attempt to subdue^ Flores before the shooting occurred, including the unsuccessful use of a stun gun. Pamerleau said the findings of the investigation will be turned over to the Bexar County district attorney’s^ office for a determination on whether charges will be filed. “We’re working this as diligently and as expeditiously^ as we can,”^ she said. \n"
     ]
    }
   ],
   "source": [
    "i = 334\n",
    "prt('H',Y_test[i])\n",
    "prt('D',X_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-75c635db019e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0ml2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_tqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTQDMCallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTQDMNotebookCallback\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras_tqdm'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Merge\n",
    "from keras_tqdm import TQDMCallback, TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed weight initialization\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer = l2(weight_decay) if weight_decay else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# adding embeddings to model ##############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \n",
      "C:\\Users\\risha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(40000, 100, input_length=50, weights=[array([[-..., mask_zero=True, name=\"embedding_1\", embeddings_regularizer=None)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ successfully added embeddings to model #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=True, name=\"lstm_1\", kernel_regularizer=None, bias_regularizer=None, recurrent_regularizer=None, dropout=0, recurrent_dropout=0)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#\n",
      "layer 0done in : 0.6754755973815918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=True, name=\"lstm_2\", kernel_regularizer=None, bias_regularizer=None, recurrent_regularizer=None, dropout=0, recurrent_dropout=0)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#\n",
      "layer 1done in : 0.5644974708557129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=True, name=\"lstm_3\", kernel_regularizer=None, bias_regularizer=None, recurrent_regularizer=None, dropout=0, recurrent_dropout=0)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#*#\n",
      "layer 2done in : 0.4738430976867676\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model = Sequential()\n",
    "print(\"############# adding embeddings to model ##############\")\n",
    "model.add(Embedding(vocab_size, embedding_size,\n",
    "                    input_length=maxlen,\n",
    "                    W_regularizer=regularizer, dropout=p_emb, weights=[embedding], mask_zero=True,\n",
    "                    name='embedding_1'))\n",
    "print('############ successfully added embeddings to model #############')\n",
    "for i in range(rnn_layers):\n",
    "    start = time.time()\n",
    "    lstm = LSTM(rnn_size, return_sequences=True, # batch_norm=batch_norm,\n",
    "                W_regularizer=regularizer, U_regularizer=regularizer,\n",
    "                b_regularizer=regularizer, dropout_W=p_W, dropout_U=p_U,\n",
    "                name='lstm_%d'%(i+1)\n",
    "                  )\n",
    "    model.add(lstm)\n",
    "    model.add(Dropout(p_dense,name='dropout_%d'%(i+1)))\n",
    "    print('*#'*30)\n",
    "    print('layer ' + str(i) + 'done in : ' + str(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Lambda\n",
    "import keras.backend as K\n",
    "\n",
    "def simple_context(X, mask, n=40, maxlend=maxlend, maxlenh=maxlenh):\n",
    "    desc, head = X[:,:maxlend,:], X[:,maxlend:,:]\n",
    "    head_activations, head_words = head[:,:,:n], head[:,:,n:]\n",
    "    desc_activations, desc_words = desc[:,:,:n], desc[:,:,n:]\n",
    "    \n",
    "    # RTFM http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.batched_tensordot\n",
    "    # activation for every head word and every desc word\n",
    "    activation_energies = K.batch_dot(head_activations, desc_activations, axes=(2,2))\n",
    "    # make sure we dont use description words that are masked out\n",
    "    activation_energies = activation_energies + -1e20*K.expand_dims(1.-K.cast(mask[:, :maxlend],'float32'),1)\n",
    "    \n",
    "    # for every head word compute weights for every desc word\n",
    "    activation_energies = K.reshape(activation_energies,(-1,maxlend))\n",
    "    activation_weights = K.softmax(activation_energies)\n",
    "    activation_weights = K.reshape(activation_weights,(-1,maxlenh,maxlend))\n",
    "\n",
    "    # for every head word compute weighted average of desc words\n",
    "    desc_avg_word = K.batch_dot(activation_weights, desc_words, axes=(2,1))\n",
    "    return K.concatenate((desc_avg_word, head_words))\n",
    "\n",
    "\n",
    "class SimpleContext(Lambda):\n",
    "    \n",
    "    def __init__(self,**kwargs):\n",
    "        super(SimpleContext, self).__init__(simple_context,**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return input_mask[:, maxlend:]\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        nb_samples = input_shape[0]\n",
    "        n = 2*(rnn_size - activation_rnn_size)\n",
    "        return (nb_samples, maxlenh, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(40000, name=\"timedistributed_1\", kernel_regularizer=None, bias_regularizer=None)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "activation_rnn_size = 40\n",
    "model.add(Lambda(simple_context,\n",
    "                 mask=lambda inputs, mask: mask[:, maxlend:],\n",
    "                 output_shape=lambda input_shape: (input_shape[0], maxlenh, 2 * (rnn_size - activation_rnn_size)),\n",
    "                 name='simplecontext_1'))\n",
    "model.add(TimeDistributed(Dense(vocab_size,\n",
    "                                W_regularizer=regularizer, b_regularizer=regularizer,\n",
    "                                name = 'timedistributed_1')))\n",
    "model.add(Activation('softmax', name='activation_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop # usually I prefer Adam but article used rmsprop\n",
    "# opt = Adam(lr=LR)  # keep calm and reduce learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr,np.float32(LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           4000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 512)           1255424   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50, 512)           0         \n",
      "_________________________________________________________________\n",
      "simplecontext_1 (Lambda)     (None, 25, 944)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 25, 40000)         37800000  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 25, 40000)         0         \n",
      "=================================================================\n",
      "Total params: 47,253,824\n",
      "Trainable params: 47,253,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_shape(x):\n",
    "    return 'x'.join(map(str,x.shape))\n",
    "    \n",
    "def inspect_model(model):\n",
    "    for i,l in enumerate(model.layers):\n",
    "        print(i, 'cls=%s name=%s'%(type(l).__name__, l.name))\n",
    "        weights = l.get_weights()\n",
    "        for weight in weights:\n",
    "            print(str_shape(weight),end=' ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cls=Embedding name=embedding_1\n",
      "40000x100 \n",
      "1 cls=LSTM name=lstm_1\n",
      "100x2048 512x2048 2048 \n",
      "2 cls=Dropout name=dropout_1\n",
      "\n",
      "3 cls=LSTM name=lstm_2\n",
      "512x2048 512x2048 2048 \n",
      "4 cls=Dropout name=dropout_2\n",
      "\n",
      "5 cls=LSTM name=lstm_3\n",
      "512x2048 512x2048 2048 \n",
      "6 cls=Dropout name=dropout_3\n",
      "\n",
      "7 cls=Lambda name=simplecontext_1\n",
      "\n",
      "8 cls=TimeDistributed name=time_distributed_1\n",
      "944x40000 40000 \n",
      "9 cls=Activation name=activation_1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FN1 and os.path.exists('data/%s.hdf5'%FN1):\n",
    "    model.load_weights('data/%s.hdf5'%FN1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpadd(x, maxlend=maxlend, eos=eos):\n",
    "    \"\"\"left (pre) pad a description to maxlend and then add eos.\n",
    "    The eos is the input to predicting the first word in the headline\n",
    "    \"\"\"\n",
    "    assert maxlend >= 0\n",
    "    if maxlend == 0:\n",
    "        return [eos]\n",
    "    n = len(x)\n",
    "    if n > maxlend:\n",
    "        x = x[-maxlend:]\n",
    "        n = maxlend\n",
    "    return [empty]*(maxlend-n) + x + [eos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [lpadd([3]*26)]\n",
    "# pad from right (post) so the first maxlend will be description followed by headline\n",
    "data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(data[:,maxlend] == eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 50), <map at 0x150764d3fd0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, map(len, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25, 40000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict(data, verbose=0, batch_size=1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variation to https://github.com/ryankiros/skip-thoughts/blob/master/decoding/search.py\n",
    "def beamsearch(predict, start=[empty]*maxlend + [eos],\n",
    "               k=1, maxsample=maxlen, use_unk=True, empty=empty, eos=eos, temperature=1.0):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "    def sample(energy, n, temperature=temperature):\n",
    "        \"\"\"sample at most n elements according to their energy\"\"\"\n",
    "        n = min(n,len(energy))\n",
    "        prb = np.exp(-np.array(energy) / temperature )\n",
    "        res = []\n",
    "        for i in range(n):\n",
    "            z = np.sum(prb)\n",
    "            r = np.argmax(np.random.multinomial(1, prb/z, 1))\n",
    "            res.append(r)\n",
    "            prb[r] = 0. # make sure we select each element only once\n",
    "        return res\n",
    "\n",
    "    dead_k = 0 # samples that reached eos\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_k = 1 # samples that did not yet reached eos\n",
    "    live_samples = [list(start)]\n",
    "    live_scores = [0]\n",
    "\n",
    "    while live_k:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        probs = predict(live_samples, empty=empty)\n",
    "        print(probs.shape)\n",
    "\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "#         cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
    "        cand_scores = np.array(live_scores)[:] - np.log(probs)\n",
    "#         cand_scores[:,empty] = 1e20\n",
    "        cand_scores[:] = 1e20\n",
    "        if not use_unk:\n",
    "            for i in range(nb_unknown_words):\n",
    "                cand_scores[:,vocab_size - 1 - i] = 1e20\n",
    "        live_scores = list(cand_scores.flatten())\n",
    "        \n",
    "\n",
    "        # find the best (lowest) scores we have from all possible dead samples and\n",
    "        # all live samples and all possible new words added\n",
    "        scores = dead_scores + live_scores\n",
    "        ranks = sample(scores, k)\n",
    "        n = len(dead_scores)\n",
    "        ranks_dead = [r for r in ranks if r < n]\n",
    "        ranks_live = [r - n for r in ranks if r >= n]\n",
    "        \n",
    "        dead_scores = [dead_scores[r] for r in ranks_dead]\n",
    "        dead_samples = [dead_samples[r] for r in ranks_dead]\n",
    "        \n",
    "        live_scores = [live_scores[r] for r in ranks_live]\n",
    "\n",
    "        # append the new words to their appropriate live sample\n",
    "        voc_size = probs.shape[1]\n",
    "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_live]\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        # even if len(live_samples) == maxsample we dont want it dead because we want one\n",
    "        # last prediction out of it to reach a headline of maxlenh\n",
    "        zombie = [s[-1] == eos or len(s) > maxsample for s in live_samples]\n",
    "        \n",
    "        # add zombies to the dead\n",
    "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]\n",
    "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
    "        dead_k = len(dead_samples)\n",
    "        # remove zombies from the living \n",
    "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
    "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
    "        live_k = len(live_samples)\n",
    "\n",
    "    return dead_samples + live_samples, dead_scores + live_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_rnn_predict(samples, empty=empty, model=model, maxlen=maxlen):\n",
    "    \"\"\"for every sample, calculate probability for every possible label\n",
    "    you need to supply your RNN model and maxlen - the length of sequences it can handle\n",
    "    \"\"\"\n",
    "    sample_lengths = map(len, samples)\n",
    "    assert all(l > maxlend for l in sample_lengths)\n",
    "    assert all(l[maxlend] == eos for l in samples)\n",
    "    # pad from right (post) so the first maxlend will be description followed by headline\n",
    "    data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "    probs = model.predict(data, verbose=0, batch_size=batch_size)\n",
    "    res = np.array([prob[sample_length-maxlend-1] for prob, sample_length in zip(probs, sample_lengths)])\n",
    "    print(res.shape)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_fold(xs):\n",
    "    \"\"\"convert list of word indexes that may contain words outside vocab_size to words inside.\n",
    "    If a word is outside, try first to use glove_idx2idx to find a similar word inside.\n",
    "    If none exist then replace all accurancies of the same unknown word with <0>, <1>, ...\n",
    "    \"\"\"\n",
    "    xs = [x if x < oov0 else glove_idx2idx.get(x,x) for x in xs]\n",
    "    # the more popular word is <0> and so on\n",
    "    outside = sorted([x for x in xs if x >= oov0])\n",
    "    # if there are more than nb_unknown_words oov words then put them all in nb_unknown_words-1\n",
    "    outside = dict((x,vocab_size-1-min(i, nb_unknown_words-1)) for i, x in enumerate(outside))\n",
    "    xs = [outside.get(x,x) for x in xs]\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_unfold(desc,xs):\n",
    "    # assume desc is the unfolded version of the start of xs\n",
    "    unfold = {}\n",
    "    for i, unfold_idx in enumerate(desc):\n",
    "        fold_idx = xs[i]\n",
    "        if fold_idx >= oov0:\n",
    "            unfold[fold_idx] = unfold_idx\n",
    "    return [unfold.get(x,x) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import Levenshtein\n",
    "\n",
    "def gensamples(skips=2, k=10, batch_size=batch_size, short=True, temperature=1., use_unk=True):\n",
    "    i = random.randint(0,len(X_test)-1)\n",
    "    print('HEAD:',' '.join(idx2word[w] for w in Y_test[i][:maxlenh]))\n",
    "    print('DESC:',' '.join(idx2word[w] for w in X_test[i][:maxlend]))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print('HEADS:')\n",
    "    x = X_test[i]\n",
    "    samples = []\n",
    "    if maxlend == 0:\n",
    "        skips = [0]\n",
    "    else:\n",
    "        skips = range(min(maxlend,len(x)), max(maxlend,len(x)), abs(maxlend - len(x)) // skips + 1)\n",
    "    for s in skips:\n",
    "        start = lpadd(x[:s])\n",
    "        fold_start = vocab_fold(start)\n",
    "        sample, score = beamsearch(predict=keras_rnn_predict, start=fold_start, k=k, temperature=temperature, use_unk=use_unk)\n",
    "        assert all(s[maxlend] == eos for s in sample)\n",
    "        samples += [(s,start,scr) for s,scr in zip(sample,score)]\n",
    "\n",
    "    samples.sort(key=lambda x: x[-1])\n",
    "    codes = []\n",
    "    for sample, start, score in samples:\n",
    "        code = ''\n",
    "        words = []\n",
    "        sample = vocab_unfold(start, sample)[len(start):]\n",
    "        for w in sample:\n",
    "            if w == eos:\n",
    "                break\n",
    "            words.append(idx2word[w])\n",
    "            code += chr(w//(256*256)) + chr((w//256)%256) + chr(w%256)\n",
    "        if short:\n",
    "            distance = min([100] + [-Levenshtein.jaro(code,c) for c in codes])\n",
    "            if distance > -0.6:\n",
    "                print(score, ' '.join(words))\n",
    "        #         print '%s (%.2f) %f'%(' '.join(words), score, distance)\n",
    "        else:\n",
    "                print(score, ' '.join(words))\n",
    "        codes.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD: 9 modern arcade games you don't have to leave your couch to play\n",
      "DESC: Most modern gamers have fond memories of childhoods^ spent visiting arcades.^ For as long as the quarters held out, those games offered hours of entertainment\n",
      "HEADS:\n",
      "(0,)\n",
      "(0,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-a07aee9f3578>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgensamples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-6adf02d84fc9>\u001b[0m in \u001b[0;36mgensamples\u001b[1;34m(skips, k, batch_size, short, temperature, use_unk)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlpadd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mfold_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab_fold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeamsearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras_rnn_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfold_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_unk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_unk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmaxlend\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0meos\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-b5b8553b3c2d>\u001b[0m in \u001b[0;36mbeamsearch\u001b[1;34m(predict, start, k, maxsample, use_unk, empty, eos, temperature)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# append the new words to their appropriate live sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mvoc_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mlive_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlive_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mvoc_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mvoc_size\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mranks_live\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "gensamples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_headline(x, nflips=None, model=None, debug=False):\n",
    "    \"\"\"given a vectorized input (after `pad_sequences`) flip some of the words in the second half (headline)\n",
    "    with words predicted by the model\n",
    "    \"\"\"\n",
    "    if nflips is None or model is None or nflips <= 0:\n",
    "        return x\n",
    "    \n",
    "    batch_size = len(x)\n",
    "    assert np.all(x[:,maxlend] == eos)\n",
    "    probs = model.predict(x, verbose=0, batch_size=batch_size)\n",
    "    x_out = x.copy()\n",
    "    for b in range(batch_size):\n",
    "        # pick locations we want to flip\n",
    "        # 0...maxlend-1 are descriptions and should be fixed\n",
    "        # maxlend is eos and should be fixed\n",
    "        flips = sorted(random.sample(range(maxlend+1,maxlen), nflips))\n",
    "        if debug and b < debug:\n",
    "            print(b, end=' ')\n",
    "        for input_idx in flips:\n",
    "            if x[b,input_idx] == empty or x[b,input_idx] == eos:\n",
    "                continue\n",
    "            # convert from input location to label location\n",
    "            # the output at maxlend (when input is eos) is feed as input at maxlend+1\n",
    "            label_idx = input_idx - (maxlend+1)\n",
    "            prob = probs[b, label_idx]\n",
    "            w = prob.argmax()\n",
    "            if w == empty:  # replace accidental empty with oov\n",
    "                w = oov0\n",
    "            if debug and b < debug:\n",
    "                print('%s => %s'%(idx2word[x_out[b,input_idx]],idx2word[w]), end=' ')\n",
    "            x_out[b,input_idx] = w\n",
    "        if debug and b < debug:\n",
    "            print()\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_seq_labels(xds, xhs, nflips=None, model=None, debug=False):\n",
    "    \"\"\"description and hedlines are converted to padded input vectors. headlines are one-hot to label\"\"\"\n",
    "    batch_size = len(xhs)\n",
    "    assert len(xds) == batch_size\n",
    "    x = [vocab_fold(lpadd(xd)+xh) for xd,xh in zip(xds,xhs)]  # the input does not have 2nd eos\n",
    "    x = sequence.pad_sequences(x, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "    x = flip_headline(x, nflips=nflips, model=model, debug=debug)\n",
    "    \n",
    "    y = np.zeros((batch_size, maxlenh, vocab_size))\n",
    "    for i, xh in enumerate(xhs):\n",
    "        xh = vocab_fold(xh) + [eos] + [empty]*maxlenh  # output does have a eos at end\n",
    "        xh = xh[:maxlenh]\n",
    "        y[i,:,:] = np_utils.to_categorical(xh, vocab_size)\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(Xd, Xh, batch_size=batch_size, nb_batches=None, nflips=None, model=None, debug=False, seed=seed):\n",
    "    \"\"\"yield batches. for training use nb_batches=None\n",
    "    for validation generate deterministic results repeating every nb_batches\n",
    "    \n",
    "    while training it is good idea to flip once in a while the values of the headlines from the\n",
    "    value taken from Xh to value generated by the model.\n",
    "    \"\"\"\n",
    "    c = nb_batches if nb_batches else 0\n",
    "    while True:\n",
    "        xds = []\n",
    "        xhs = []\n",
    "        if nb_batches and c >= nb_batches:\n",
    "            c = 0\n",
    "        new_seed = random.randint(0, sys.maxsize)\n",
    "        random.seed(c+123456789+seed)\n",
    "        for b in range(batch_size):\n",
    "            t = random.randint(0,len(Xd)-1)\n",
    "\n",
    "            xd = Xd[t]\n",
    "            s = random.randint(min(maxlend,len(xd)), max(maxlend,len(xd)))\n",
    "            xds.append(xd[:s])\n",
    "            \n",
    "            xh = Xh[t]\n",
    "            s = random.randint(min(maxlenh,len(xh)), max(maxlenh,len(xh)))\n",
    "            xhs.append(xh[:s])\n",
    "\n",
    "        # undo the seeding before we yield inorder not to affect the caller\n",
    "        c+= 1\n",
    "        random.seed(new_seed)\n",
    "\n",
    "        yield conv_seq_labels(xds, xhs, nflips=nflips, model=model, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 50), (16, 25, 40000), 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = next(gen(X_train, Y_train, batch_size=batch_size))\n",
    "r[0].shape, r[1].shape, len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gen(gen, n=5):\n",
    "    Xtr,Ytr = next(gen)\n",
    "    for i in range(n):\n",
    "        assert Xtr[i,maxlend] == eos\n",
    "        x = Xtr[i,:maxlend]\n",
    "        y = Xtr[i,maxlend:]\n",
    "        yy = Ytr[i,:]\n",
    "        yy = np.where(yy)[1]\n",
    "        prt('L',yy)\n",
    "        prt('H',y)\n",
    "        if maxlend:\n",
    "            prt('D',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:crying over spilled <1>^ TPP down the <0>^ ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "H:~ crying over spilled <2>^ TPP down the <1>^ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "D:Faire turn since Thomas Mulcair became its leader. Mulcair says he is <0>^ in <3>^ of the trade deal, but with the promise that his \n",
      "L:By <0>^ Richards, The promises of God 2003. ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "H:~ By <3>^ Richards, The promises of God 2003. _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "D:I speak unto you, . . . they are <0>^ (John <2>^ Are you fascinated with <1>^ of <4>^ God’s path of life leads to \n",
      "L:Brown Water Advisory issued for Hawaii ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "H:~ Brown Water Advisory issued for Hawaii _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "D:A Brown Water Advisory has been issued Friday for Hawaii as a result of heavy rain. See the original article at: Hawaii News Now – \n",
      "L:<3>^ Preview: <1>^ Set to Rocket to No. <0>^ 'The <2>^ Opens in Imax - Hollywood Reporter ~ _ _ _ _ _ _ _ \n",
      "H:~ <5>^ Preview: <2>^ Set to Rocket to No. <0>^ 'The <3>^ Opens in Imax - Hollywood Reporter _ _ _ _ _ _ _ \n",
      "D:Pedro Salma 's space epic Gravity , facing far less competition, opened to a <1>^ <4>^ million. In November 2014, Christopher Nolan 's particles debuted \n",
      "L:tickets turn free papal tickets for hefty profits ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "H:~ tickets turn free papal tickets for hefty profits _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "D:a bit <0>^ except the eBay ad says it comes with tickets to see Pope Francis. Same with the $10 train passes marked up to \n"
     ]
    }
   ],
   "source": [
    "test_gen(gen(X_train, Y_train, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:crying over spilled <1>^ TPP down the <0>^ ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "H:~ crying over spilled <2>^ TPP down War. <1>^ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "D:Faire turn since Thomas Mulcair became its leader. Mulcair says he is <0>^ in <3>^ of the trade deal, but with the promise that his \n",
      "L:By <0>^ Richards, The promises of God 2003. ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "H:~ By <3>^ Richards, showcases promises of God FSNA _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "D:I speak unto you, . . . they are <0>^ (John <2>^ Are you fascinated with <1>^ of <4>^ God’s path of life leads to \n",
      "L:Brown Water Advisory issued for Hawaii ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "H:~ Brown Website: Advisory issued for Hawaii _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "D:A Brown Water Advisory has been issued Friday for Hawaii as a result of heavy rain. See the original article at: Hawaii News Now – \n",
      "L:<3>^ Preview: <1>^ Set to Rocket to No. <0>^ 'The <2>^ Opens in Imax - Hollywood Reporter ~ _ _ _ _ _ _ _ \n",
      "H:~ <5>^ Preview: toe Set to Rocket to No. <0>^ 'The <3>^ Opens in Imax Case Case Reporter _ _ _ _ _ _ _ \n",
      "D:Pedro Salma 's space epic Gravity , facing far less competition, opened to a <1>^ <4>^ million. In November 2014, Christopher Nolan 's particles debuted \n",
      "L:tickets turn free papal tickets for hefty profits ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "H:~ tickets turn free Website: tickets for hefty profits _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "D:a bit <0>^ except the eBay ad says it comes with tickets to see Pope Francis. Same with the $10 train passes marked up to \n"
     ]
    }
   ],
   "source": [
    "test_gen(gen(X_train, Y_train, nflips=6, model=model, debug=False, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "valgen = gen(X_test, Y_test,nb_batches=3, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:Hey Poor Podcast Episode <0>^ Super Nintendo President Maker ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "H:~ Hey Poor Podcast Episode <5>^ Super Nintendo President Maker _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "D:Mario Maker and <0>^ The <4>^ Jay <2>^ Francis <3>^ Adam Foster and Gary swallow discuss the hiring of <1>^ new company president, as well \n",
      "L:N.J. football players get probation for sexual bullying ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "H:~ N.J. football players get probation for sexual bullying _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "D:from a New Jersey high school known for its football victories have been placed on probation -- but have been spared from being labeled sex \n",
      "L:Canning candidate van Lieshout faces tough questions after court no show ~ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "H:~ Canning candidate van Lieshout faces tough questions after court no show _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "D:to be angry about what they've done to me and I don't have a problem saying that.” Ms van Lieshout has been contacted for further \n",
      "L:Hey Poor Podcast Episode <0>^ Super Nintendo President Maker ~ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "H:~ Hey Poor Podcast Episode <5>^ Super Nintendo President Maker _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "D:Mario Maker and <0>^ The <4>^ Jay <2>^ Francis <3>^ Adam Foster and Gary swallow discuss the hiring of <1>^ new company president, as well \n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    test_gen(valgen, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingen = gen(X_train, Y_train, batch_size=batch_size, nflips=nflips, model=model)\n",
    "valgen = gen(X_test, Y_test, nb_batches=nb_val_samples//batch_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 50), (16, 25, 40000), 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = next(traingen)\n",
    "r[0].shape, r[1].shape, len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbCallback = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "saveModelCallback = keras.callbacks.ModelCheckpoint('./checkpoints/Iweights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\risha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., callbacks=[<keras.ca..., steps_per_epoch=10000, epochs=1, validation_steps=1000)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  114/10000 [..............................] - ETA: 3:24:22 - loss: 7.9414"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-525130c07c7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Iteration'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     h = model.fit_generator(traingen, samples_per_epoch=nb_train_samples,\n\u001b[1;32m----> 4\u001b[1;33m                         \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtbCallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaveModelCallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                            )\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2075\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2076\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1795\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1796\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1797\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(5):\n",
    "    print('Iteration', iteration)\n",
    "    h = model.fit_generator(traingen, samples_per_epoch=nb_train_samples,\n",
    "                        nb_epoch=1, validation_data=valgen, nb_val_samples=nb_val_samples, callbacks=[tbCallback, saveModelCallback]\n",
    "                           )\n",
    "    for k,v in h.history.iteritems():\n",
    "        history[k] = history.get(k,[]) + v\n",
    "    with open('data/%s.history.pkl'%FN,'wb') as fp:\n",
    "        pickle.dump(history,fp,-1)\n",
    "    model.save_weights('data/%s.hdf5'%FN, overwrite=True)\n",
    "#     gensamples(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
